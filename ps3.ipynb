{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CS 171 / EE 142 Problem Set 3\n",
    "# Due Friday, February 28, 2025\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "In this problem set, you are to implement a three-layer (3 layers of weights, 2 hidden layers of units) neural network for binary classification.  All non-linearities are to be hyperbolic tangents (tanh).\n",
    "\n",
    "Details are given below.  *Please read the **entire** notebook carefully before proceeding.*\n",
    "\n",
    "You need to both fill in the necesary code, **and** answer the question at the bottom.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your information below:\n",
    "\n",
    "<div style=\"color: #000000;background-color: #EEEEFF\">\n",
    "    Your Name (submitter): <br>\n",
    "Your student ID (submitter):\n",
    "    \n",
    "<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <font size=+2>Total Problem Set Grading</font> (to be completed by grader)<br>\n",
    "    Total Points: /20<br>\n",
    "    Late Days Used on this Assignment: <br>\n",
    "    Total Late Days Used: <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Below are the only imports that are necessary (or allowed)\n",
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "We will be using a USPS digit dataset (provided in the file uspsall73.mat).\n",
    "It has 16-by-16 grayscale images of each of the 10 different hand-written digits\n",
    "However, we will load only two of the digits to use as the two classes in\n",
    "binary classification (the sevens and the nines).  The code below already z-score \n",
    "normalizes the data for you -- an important first step!\n",
    "<p>\n",
    "    <p>\n",
    "    Note:  digit recognition typically uses larger datasets (more than 1000 images per class) and often with higher resolution images.  We are using lower resolution images (fewer features) and small training and validation data sets (fewer examples) to speed up training time.  You may choose to further reduce the training data set during your own debugging to speed things up.  Be sure to remove that reduction and rerun the notebook with the full dataset once you have finished debugging.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to load two of the 10 classes (c1 is for Y=-1 and c2 is for Y=+1)\n",
    "def loaddigitdata(c1,c2,m):\n",
    "    f = h5py.File('/usr/local/cs171/uspsall73.mat','r') \n",
    "    data = f.get('data') \n",
    "    data = np.array(data).astype(float)\n",
    "    X = np.concatenate((data[c1,:,:],data[c2,:,:]))\n",
    "    Y = np.concatenate((-np.ones((data.shape[1])),np.ones((data.shape[1]))))\n",
    "    \n",
    "    rs = np.random.RandomState(seed=132857) # setting seed so that dataset is consistent\n",
    "    p = rs.permutation(X.shape[0])\n",
    "    X = X[p] # this and next line make copies, but that's okay given how small our dataset is\n",
    "    Y = Y[p]\n",
    "    \n",
    "    trainX = X[0:m,:] # use the first m (after shuffling) for training\n",
    "    trainY = Y[0:m,np.newaxis]\n",
    "    validX = X[m:,:] # use the rest for validation\n",
    "    validY = Y[m:,np.newaxis]\n",
    "    return (trainX,trainY,validX,validY)\n",
    "\n",
    "# In case you care (not necessary for the assignment)\n",
    "def drawexample(x,ax=None): # takes an x *vector* and draws the image it encodes\n",
    "    if ax is None:\n",
    "        plt.imshow(np.reshape(x,(16,16)).T,cmap='gist_yarg')\n",
    "    else:\n",
    "        ax.imshow(np.reshape(x,(16,16)).T,cmap='gist_yarg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data, to differentiate between 7s and 9s\n",
    "# we will use on 1100 examples for training (50% of the data) and the other half for hold-out validation\n",
    "(trainX,trainY,validX,validY) = loaddigitdata(6,8,1100)\n",
    "means = trainX.mean(axis=0)\n",
    "stddevs = trainX.std(axis=0)\n",
    "stddevs[stddevs<1e-6] = 1.0\n",
    "trainX = (trainX-means)/stddevs # z-score normalization\n",
    "validX = (validX-means)/stddevs # apply same transformation to validation set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert this cell to a code cell if you wish to see each of the examples, plotted\n",
    "# (completely not necessary for the problem set)\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8,8)\n",
    "\n",
    "ax = f.add_subplot(111)\n",
    "plt.ion()\n",
    "f.canvas.draw()\n",
    "for exi in range(trainX.shape[0]):\n",
    "    display.clear_output(wait=True)\n",
    "    drawexample(trainX[exi,:])\n",
    "    digitid = (9 if trainY[exi]>0 else 7)\n",
    "    ax.set_title('y = '+str(int(trainY[exi]))+\" [\"+str(digitid)+\"]\")\n",
    "    display.display(f)\n",
    "    #time.sleep(0.1)\n",
    "    \n",
    "drawexample(trainX[0])  # Display the first image\n",
    "plt.show()  # Ensure the image is rendered\n",
    "print(\"Label:\", trainY[0]) \n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 1:</font> <font size=+1>(17 points)</font>\n",
    "This is the main portion of the assignment.  Read the text below carefully.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <font size=+1>Grading</font> (to be completed by grader)<br>\n",
    "    Score: /17<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE `nneval` and `trainneuralnet`\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "\n",
    "Note that the target $Y$ values are +1 and -1.  The internal nodes should all use the hyperbolic tangent (np.tanh) as the nonlinearity.\n",
    "\n",
    "You need to write the two functions below (plus any more you would like to add to help): `nneval` and `trainneuralnet`.  The first takes an array/matrix of X vectors and the weights from a neural network and returns a vector of discriminant values (should be numbers between -infinity and +infinity).  The second takes a data set (Xs and Ys), the number of hidden units, and the lambda value (for l2-regularization), and returns the weights, Wts.  Wts[0] are the weights from the input to the first hidden layer.  Wts[1] are the weights from the first hidden layer to the second hidden layer.  Wts[2] are the weights from the second hidden layer to the output.\n",
    "\n",
    "A few notes:\n",
    "- **Starting Weights**: The code supplied randomly selects the weights near zero.  Technically, this is called Xavier initialization (see https://ai.stackexchange.com/questions/42975/what-are-the-techniques-used-to-initialize-weights-for-neural-networks) for an interactive explanation.  But for the purposes of the assignment, you can just accept this is a good way to initialize neural network weights.\n",
    "- **Offset Terms**: Each layer should have an \"offset\" or \"intercept\" unit (to supply a 1 to the next layer), except the output layer.\n",
    "- **Batch Updates**: For a problem this small, use batch updates.  That is, the step is based on the sum of the gradients for each data point in the training set.\n",
    "- **Step Size**: https://www.digitalocean.com/community/tutorials/intro-to-optimization-momentum-rmsprop-adam describes a number of methods to adaptively control $\\eta$ for fast convergence.  You don't need to understand any of them; however, without them, convergence to good solutions on this problem can be quite slow.  Therefore, *use RMSprop*: the code below has a simple version of RMSprop that is sufficient for this assignment.  You need to supply the code that calculates `sumofgrad2` which should be the sum of the square of each element of the *full* gradient (the squared length of the gradient including the regularization term).  (for debugging, feel free to use a constant $\\eta$ initially). \n",
    "- **Stopping Criterion**: To determine when to stop, check the loss (full loss, including the regularization term) function every 10 iterations.  If it has not improved by at least $10^{-10}$ over those 10 iterations, stop.\n",
    "- **Regularization**: You should penalize (from the regularization) all of the weights, even those coming out of offset units.  While it makes sense sometimes not to penalize the ones for the constant $1$ units, you'll find this easier if you just penalize them all.\n",
    "\n",
    "Tips that might help:\n",
    "- Display the loss function's value every 10 iterations (or so).  It should be getting smaller.  If not, your gradient is not pointing in the right direction.\n",
    "    - The most common mistake is to subtly have the wrong loss function or the wrong gradient.  The gradient is the gradient of the entire loss function, including the regularization term.  A small mistake will cause the loss function to initially get better, but then to get worse.\n",
    "- The smaller $\\lambda$ is and the more units, the more difficult (longer) the optimization will be.\n",
    "- Write a function to do forward propagation and one to do backward propagation.  Write a function to evaluate the loss function.  In general, break things up to keep things straight.\n",
    "- Processing the entire batch at once is more efficient in numpy than using a loop over the examples.  Use numpy broadcasting to avoid loops where possible.\n",
    "- Create a small dataset from the examples in class.  Check that you get the same results for forward propagation.  Check you get the same results for backward propagation.  Check that you get the same results for the gradients.\n",
    "- You can reduce the stopping criterion (from $10^{-10}$ to something closer to 1) temporarily to make it finish faster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm defining helper functions here\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "# Forward propagation without weight initialization\n",
    "def forward_propagation(X, Wts):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    W1, W2, W3 = Wts  # Use pre-initialized weights\n",
    "    \n",
    "    # Add bias column to input\n",
    "    a0 = np.hstack((np.ones((m, 1)), X))\n",
    "\n",
    "    # First hidden layer\n",
    "    z1 = np.dot(a0, W1.T)\n",
    "    a1 = tanh(z1)\n",
    "    a1 = np.hstack((np.ones((a1.shape[0], 1)), a1))\n",
    "\n",
    "    # Second hidden layer\n",
    "    z2 = np.dot(a1, W2.T)\n",
    "    a2 = tanh(z2)\n",
    "    a2 = np.hstack((np.ones((a2.shape[0], 1)), a2))\n",
    "\n",
    "    # Output layer\n",
    "    z3 = np.dot(a2, W3.T)\n",
    "    a3 = tanh(z3)\n",
    "\n",
    "    return z1, a1, z2, a2, z3, a3\n",
    "\n",
    "def nneval(X, Wts):\n",
    "    _, _, _, _, _, a3 = forward_propagation(X, Wts)\n",
    "    return a3  # Output the necessary values. \n",
    "\n",
    "        ## Your code here\n",
    "    \n",
    "    # Your functions need only work for neural networks of exactly 3 layers of weights\n",
    "    # This training function has a single scalar parameter, nhid, to indicate the number of\n",
    "    # hidden units.  This is the number in the first hidden layer.  The second hidden layer will have 1/2 this number\n",
    "\n",
    "# Backpropagation\n",
    "def backward_propagation(Y, z1, a1, z2, a2, z3, Wts):\n",
    "    W1, W2, W3 = Wts\n",
    "\n",
    "    # Compute delta_z3 (output layer error) using tanh derivative\n",
    "    delta_z3 = (1 - tanh(z3)) * (-Y)\n",
    "\n",
    "    # Backpropagate to second hidden layer\n",
    "    delta_a2 = delta_z3 @ W3[:, 1:]\n",
    "    delta_z2 = delta_a2 * tanh_derivative(z2)\n",
    "\n",
    "    # Backpropagate to first hidden layer\n",
    "    delta_a1 = delta_z2 @ W2[:, 1:]\n",
    "    delta_z1 = delta_a1 * tanh_derivative(z1)\n",
    "\n",
    "    return delta_z3, delta_a2, delta_z2, delta_a1, delta_z1\n",
    "\n",
    "\n",
    "def trainneuralnet(X, Y, nhid, lam):\n",
    "    max_its = 10000\n",
    "    tol = 1e-10\n",
    "    (m, n) = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    W1 = (np.random.rand(nhid, n + 1) * 2 - 1) * np.sqrt(6.0 / (n + nhid + 1))\n",
    "    W2 = (np.random.rand(nhid // 2, nhid + 1) * 2 - 1) * np.sqrt(6.0 / (nhid + nhid // 2 + 1))\n",
    "    W3 = (np.random.rand(1, nhid // 2 + 1) * 2 - 1) * np.sqrt(6.0 / (nhid // 2 + 2))\n",
    "\n",
    "    W1[:, 0] = 0\n",
    "    W2[:, 0] = 0\n",
    "    W3[:, 0] = 0\n",
    "\n",
    "    Wts = [W1, W2, W3]\n",
    "    \n",
    "    iteration = 0\n",
    "    # keep this:\n",
    "    Eg2 = 1\n",
    "    loss_history = []\n",
    "\n",
    "    while iteration < max_its:\n",
    "        z1, a1, z2, a2, z3, a3 = forward_propagation(X, Wts)\n",
    "        loss = np.mean((a3 - Y) ** 2)\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        # Check stopping condition every 10 iterations\n",
    "        if iteration >= 10 and abs(loss_history[-1] - loss_history[-2]) < tol:\n",
    "            break\n",
    "\n",
    "        delta_z3, delta_a2, delta_z2, delta_a1, delta_z1 = backward_propagation(Y, z1, a1, z2, a2, z3, Wts)\n",
    "\n",
    "        grad_W3 = np.dot(delta_z3.T, a2) + lam * W3\n",
    "        grad_W2 = np.dot(delta_z2.T, a1) + lam * W2\n",
    "        grad_W1 = np.dot(delta_z1.T, np.hstack((np.ones((m, 1)), X))) + lam * W1\n",
    "\n",
    "        sumofgrad2 = np.sum(grad_W1**2) + np.sum(grad_W2**2) + np.sum(grad_W3**2)\n",
    "\n",
    "        Eg2 = 0.9 * Eg2 + 0.1 * sumofgrad2\n",
    "        eta = 0.01 / (np.sqrt(1e-10 + Eg2)) \n",
    "        W1 -= eta * grad_W1\n",
    "        W2 -= eta * grad_W2\n",
    "        W3 -= eta * grad_W3\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return Wts  # Trained weights\n",
    "\n",
    "# while not done:\n",
    "\n",
    "# #         # when you need \"eta\" (the step size), do this after you have already\n",
    "# #         # calculated the gradient (but before you take a step):\n",
    "# #         # [where \"sumofgrad2\" is the sum of the squares of each element of the gradient\n",
    "# #         #  that is, you square *all* of the gradient values and then add them all together]\n",
    "# #         # [recall that this is the gradient of the full loss function that includes every\n",
    "# #         #  example *and* the regularizer term]\n",
    "# #         Eg2 = 0.9*Eg2 + 0.1*sumofgrad2\n",
    "# #         eta = 0.01/(np.sqrt((1e-10+Eg2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 3.46 s, total: 1min 12s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use this cell (or others you add) to check your network\n",
    "# I would debug on simple examples you create yourself (trying to understand what happens with\n",
    "#  the full 256-dimensional data is hard)\n",
    "\n",
    "#an example of training on the USPS data with 16/8 hidden units and lambda=0.00001, takes about 1500 iterations and about 20 seconds for the solutions\n",
    "Wts = trainneuralnet(trainX,trainY,16,1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance plot\n",
    "<div class=\"alert alert-info\">\n",
    "The code below will plot your algorithm's error rate on this data set for various regularization strengths and numbers of hidden units.\n",
    "\n",
    "Make sure your code works for this plot.\n",
    "\n",
    "My code runs in about 20 minutes (to produce the full plot below)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code added to do logistic regression, just for a comparison\n",
    "def addones(Z):\n",
    "    return np.hstack((np.ones((Z.shape[0],1)), Z))\n",
    "\n",
    "def learnlogreg_sklearn(X,Y,regstr=0.0,penalizeb=False):\n",
    "    X = addones(X)\n",
    "    Y = Y[:,0]\n",
    "    from sklearn.linear_model import LogisticRegression as lr\n",
    "    if regstr==0.0:\n",
    "        pen = 'none'\n",
    "        C = np.inf\n",
    "    else:\n",
    "        pen = 'l2'\n",
    "        C = 2.0/regstr # ???\n",
    "\n",
    "    Y = (Y>0.5).astype(int)\n",
    "    if penalizeb:\n",
    "        lrres = lr(tol=1e-6,max_iter=100000,penalty=pen,C=C,\n",
    "                   fit_intercept=False,\n",
    "                   solver='newton-cg',multi_class='multinomial').fit(X,Y)\n",
    "        w = lrres.coef_[0,:]\n",
    "    else:\n",
    "        lrres = lr(tol=1e-6,max_iter=100000,penalty=pen,C=C,\n",
    "                   fit_intercept=True,\n",
    "                   solver='newton-cg',multi_class='multinomial').fit(X[:,1:],Y)\n",
    "        w = np.hstack((lrres.intercept_,lrres.coef_[0,:]))\n",
    "    return w\n",
    "\n",
    "def predictlogreg(X,w):\n",
    "    return addones(X)@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to update the plot as new results are generated\n",
    "def setupfig():\n",
    "    f = plt.figure()\n",
    "    f.set_size_inches(8,8)\n",
    "    ax = f.add_subplot(111)\n",
    "    plt.ion()\n",
    "    f.canvas.draw()\n",
    "    return (f,ax)\n",
    "\n",
    "def plotit(lams,nhiddens,erates,f,ax):\n",
    "    ax.clear()\n",
    "    for i in range(nhiddens.shape[0]):\n",
    "        ax.plot(lams,erates[:,i],'*-')\n",
    "    ax.set_yscale('log',subs=[1,2,3,4,5,6,7,8,9])\n",
    "    ax.set_yticks([0.1,0.01])\n",
    "    ax.set_xscale('log')\n",
    "    f.canvas.draw()\n",
    "    ax.set_xlabel('lambda')\n",
    "    ax.set_ylabel('validation error rate')\n",
    "    ax.legend([(('# hidden units = '+str(x)+','+str(x//2)) if x>0 else 'logistic regression') for x in nhiddens])\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def errorrate(Y,predy):\n",
    "    predy = np.sign(predy)\n",
    "    return (predy!=Y).mean()\n",
    "    \n",
    "def multirestart(trainX,trainY,nhid,lam,ntries):\n",
    "    besterrsc = 2.0\n",
    "    for i in range(ntries):\n",
    "        Wts = trainneuralnet(trainX,trainY,nhid,lam)\n",
    "        errsc = errorrate(trainY,nneval(trainX,Wts))\n",
    "        if errsc<besterrsc:\n",
    "            returnWts = Wts\n",
    "            besterrsc = errsc\n",
    "    return returnWts\n",
    "\n",
    "nhiddens = np.array([0,2,8,16])\n",
    "lams = np.logspace(-3,3,10)\n",
    "erates = np.empty([lams.shape[0],nhiddens.shape[0]])\n",
    "erates[:,:] = np.nan\n",
    "\n",
    "(f,ax) = setupfig()\n",
    "    \n",
    "for ni, nhid in enumerate(nhiddens):\n",
    "    for li, lam in reversed(list(enumerate(lams))):\n",
    "        if nhid==0:\n",
    "            w = learnlogreg_sklearn(trainX,trainY,lam)\n",
    "            predy = predictlogreg(validX,w)[:,np.newaxis]\n",
    "        else:\n",
    "            Wts = multirestart(trainX,trainY,nhid,lam,4) \n",
    "            #Wts = trainneuralnet(trainX,trainY,nhid,lam)\n",
    "            predy = nneval(validX,Wts)\n",
    "        erates[li,ni] = errorrate(validY,predy)\n",
    "        \n",
    "        plotit(lams,nhiddens,erates,f,ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 2:</font> <font size=+1>(3 points)</font>\n",
    "How do you interpret the plot above?  How and why does the plot differ by number of hidden units?  By $\\lambda$ value?  What parts of this plot agree with the material taught?  What parts do not?\n",
    "</div>\n",
    "<div class=\"alert alert-success\">\n",
    "    <font size=+1>Grading</font> (to be completed by grader)<br>\n",
    "    Score: /3<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here I think model demonstrates the ratio of regularization and how complex a model should be. A small lambda overfits, large does the opposite. A network with more units can learn potentially complex patterns could also end up overfitting data. With less units, the opposite happens. Just like in class, low lambda shows high variance. This plot in particular fluncuates a lot though. This doesn't seem to agree with what was taught. Also addones wasn't defined so I did. Hope that is alright. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS 171",
   "language": "python",
   "name": "cs171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
